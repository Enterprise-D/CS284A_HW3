{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Enterprise-J/CS284A_HW3/blob/main/hw3_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwPY3sbFjQ9c"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvqsbyJtjQ9l"
      },
      "source": [
        "# CS184A/284A: AI in Biology and Medicine\n",
        "# HW3\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfeoP40PjQ9m"
      },
      "source": [
        "# Predicting TF Binding Sites\n",
        "\n",
        "Transcription Factors (TFs) are proteins that bind to the DNA and help regulate gene transcription. The TFs have to recognize some \"motif\" on the DNA upstream from the gene, and DNA accessibility also plays a role.\n",
        "\n",
        "In tis problem set, we will develop ML methods to predict which sequences can be bound by a transcription factor called JUND.  The binding profile of JUND expressed in terms of a sequence logo is shown in the following picture.\n",
        "\n",
        "More information on JUND can be found here:\n",
        "- https://www.genecards.org/cgi-bin/carddisp.pl?gene=JUND"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dKuWK9dwjQ9n",
        "outputId": "6b9d6540-f27a-4757-c27b-f6b82449623f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CS284A_HW3'...\n",
            "remote: Enumerating objects: 56, done.\u001b[K\n",
            "remote: Counting objects: 100% (56/56), done.\u001b[K\n",
            "remote: Compressing objects: 100% (49/49), done.\u001b[K\n",
            "remote: Total 56 (delta 14), reused 43 (delta 5), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (56/56), done.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "image/png": {
              "width": 350
            }
          },
          "execution_count": 1
        }
      ],
      "source": [
        "!git clone https://github.com/Enterprise-J/CS284A_HW3\n",
        "import numpy as np\n",
        "# get the image\n",
        "from IPython import display\n",
        "display.Image(\"https://www.ismara.unibas.ch/ISMARA/scratch/NHBE_SC2/ismara_report/logos/JUND.png\", width=350)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCSTw6X4jQ9o"
      },
      "source": [
        "\n",
        "## MLP model\n",
        "\n",
        "In this assignment you'll write an MLP model the predict whether a segments of the human chromosome 22 (Chr22) contain the binding sites for the JUND TF. You can modify the mlp notebook I shared with you to work on this problem. You need to have at least one hidden layer. You have to compute a weighted loss, and include accessibility information in your model, as described below.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiWOvfd3jQ9o"
      },
      "source": [
        "\n",
        "## Dataset\n",
        "The data comprises 101 length segments from Chr22, with each position a one-hot vector denoting one of the four bases (A, C, G, T). Thus, each element of the input is 2d with size 101×4. Each such element has a target label 0 or 1, indicating whether the TF binds to that segment or not. The data also includes a weight per input element, since there are only a few binding sites (0.42%), so that you'd obtain an accuracy of 99.58% just by predicting there are no binding sites. This means you have to use the weights to discount the losses for label 0 and enhance the losses for label 1 items. Finally, there is an array of values, one per input element, that also indicates the chromosome accessibility for that segment.\n",
        "\n",
        "\n",
        "Data Credit: Mohammed Zaki"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DTmz1ifjQ9p"
      },
      "source": [
        "## Download data\n",
        "\n",
        "\n",
        "\n",
        "*   First, you need to download data file named \"TF_data.zip\" from Canvas. Unzip it and create the train, valid, test directories.\n",
        "\n",
        "*   If you use Google Colab, you can first upload the TF_data.zip file, and then run the follow command\n",
        "\n",
        "<center><code>!unzip TF_data.zip</code></center>\n",
        "\n",
        "*   The data is split into training, validation and testing sets. Each set contains the following files:\n",
        "  * shard-0-X.joblib: the set of 101 x 4 input elements\n",
        "  * shard-0-y.joblib: the true labels: 0 or 1\n",
        "  * shard-0-w.joblib: weight per input element\n",
        "  * shard-0-a.joblib: accessibility value per input element\n",
        "\n",
        "*   After unzip the data file, you  can read these files by using joblib.load function, which will populate a numpy array. For example\n",
        "<center><code>X = joblib.load('shard-0-X.joblib')</code></center>\n",
        "will results in a numpy array X, which you can then convert to torch tensor, and so on.\n",
        "\n",
        "*  The roles of training, validation and testing sets:\n",
        "  * Use training set to tune the parameters of the model.\n",
        "  * Use validation set to select model structure and hyperparamters (e.g., number of epochs, learning rate, etc).\n",
        "  * Use test set for the final evaluation.  You should never touch test set for either your model training or model selection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "aU4qdBYNjQ9p"
      },
      "outputs": [],
      "source": [
        "# Uncomment the following command if you run this code in Google Colab\n",
        "#!unzip TF_data.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "OeeY-xOmjQ9q"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import joblib\n",
        "from sklearn.metrics import precision_score, recall_score, confusion_matrix, f1_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRbykfYvjQ9q"
      },
      "source": [
        "## Set up DataLoader for fetching training and testing data\n",
        "\n",
        "Because we use mini-batch gradient descent for training.  We need to set up dataloader that can provide us a minibatch of data samples.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "lL3_9L8mjQ9q"
      },
      "outputs": [],
      "source": [
        "from torch._C import dtype\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class JUND_Dataset(Dataset):\n",
        "    def __init__(self, data_dir):\n",
        "        '''load X, y, w, a from data_dir'''\n",
        "        super(JUND_Dataset, self).__init__()\n",
        "\n",
        "        # load X, y, w, a from given data_dir\n",
        "        # convert them into torch tensors\n",
        "        self.X = torch.from_numpy(joblib.load(data_dir + '/shard-0-X.joblib')).float()\n",
        "        self.y = torch.from_numpy(joblib.load(data_dir + '/shard-0-y.joblib')).float()\n",
        "        self.w = torch.from_numpy(joblib.load(data_dir + '/shard-0-w.joblib')).float()\n",
        "        self.a = torch.from_numpy(joblib.load(data_dir + '/shard-0-a.joblib')).float()\n",
        "\n",
        "    def __len__(self):\n",
        "        '''return len of dataset'''\n",
        "        return self.X.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        '''return X, y, w, and a values at index idx'''\n",
        "        return self.X[idx],self.y[idx],self.w[idx], self.a[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "if_tzzRDjQ9r"
      },
      "outputs": [],
      "source": [
        "# get data\n",
        "#\n",
        "# You may need to change the directory if the data are not stored under current directory\n",
        "#\n",
        "train_dataset = JUND_Dataset('CS284A_HW3/train_dataset')\n",
        "test_dataset = JUND_Dataset('CS284A_HW3/test_dataset')\n",
        "valid_dataset = JUND_Dataset('CS284A_HW3/valid_dataset')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGjDB3LXjQ9r"
      },
      "source": [
        "### Traing and test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "id": "yyy7M9tRjQ9r"
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "\n",
        "# Data loader\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False)\n",
        "valid_loader = torch.utils.data.DataLoader(dataset=valid_dataset,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "3yUe4QO_jQ9s",
        "outputId": "e42a691b-2bc5-4cff-d490-2c68b8be44ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train data:  276216\n",
            "Test data:  34528\n",
            "Valid data:  34527\n"
          ]
        }
      ],
      "source": [
        "print('Train data: ', len(train_dataset))\n",
        "print('Test data: ', len(test_dataset))\n",
        "print('Valid data: ', len(valid_dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_zJVxnrjQ9s"
      },
      "source": [
        "Your result should look like:\n",
        "\n",
        "```\n",
        "Train data:  276216\n",
        "Test data:  34528\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qqq8oCBtjQ9s"
      },
      "source": [
        "### Fetch a minibatch and check the size of the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBbTOt4fjQ9t"
      },
      "outputs": [],
      "source": [
        "X,y,w,a = next(iter(train_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GER6uK7kjQ9t",
        "outputId": "f139497a-c91a-4654-e8b5-c18c2cc3293a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": "(torch.Size([512, 101, 4]),\n torch.Size([512, 1]),\n torch.Size([512, 1]),\n torch.Size([512, 1]))"
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# run the following code to check the size of data in each minibatch\n",
        "X.shape, y.shape, w.shape, a.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gsf5nYlWjQ9t"
      },
      "source": [
        "Your result should look like the following:\n",
        "\n",
        "```\n",
        "(torch.Size([100, 101, 4]),\n",
        " torch.Size([100, 1]),\n",
        " torch.Size([100, 1]),\n",
        " torch.Size([100, 1]))\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twb1klQtjQ9u"
      },
      "source": [
        "# Problem 1 - MLP\n",
        "\n",
        "Define an MLP (multi-layer perceptron) with at least one hidden layer to predict the labels given inputs.\n",
        "\n",
        "Please note the following:\n",
        "\n",
        "- The label for each input is either 0 or 1, so this is essentially a binary classification problem.\n",
        "- Input consist of both X and a:\n",
        "  - X: represents the DNA sequence. Each position is a one-hot vector denoting one of the four bases (A, C, G, T). Thus, each element of the input is 2d with size 101×4.   Since each input is treated as a vector in MLP, the 2d array needs to be flattened into a 404-dimensional vector.\n",
        "  - a: represents the chromatin accessiblity of the input DNA sequence segment. You can think of \"a\" as an additional feature for each input. You can decide how to use it.  For instance, if you are using hidden dimension of 128, then after concatenating the accessibility value, it will become a 129d vector, which should be fed to the final output layer of size 1, since we have a binary class/label.\n",
        "\n",
        "An initial template code, representing a simple model, is provided.  Your job is to change the definition of the model to improve the model's performance on the test dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7N_R33XjQ9u"
      },
      "outputs": [],
      "source": [
        "###############################################################################\n",
        "#  modify this code block\n",
        "###############################################################################\n",
        "# MLP\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self, input_size=101*4, hidden_size=45):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_size),\n",
        "            nn.ReLU())\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.BatchNorm1d(hidden_size+1),\n",
        "            nn.Linear(hidden_size+1, 1))\n",
        "\n",
        "    def forward(self, X, a):\n",
        "        out = X.reshape(X.size(0),-1)\n",
        "        out = self.layer1(out)\n",
        "        out = torch.cat((out, a),1)\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "# end of model definition\n",
        "###############################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25BxUCN7jQ9u"
      },
      "source": [
        "### Have a test run of your model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BVKiff6LjQ9v"
      },
      "outputs": [],
      "source": [
        "model = MyModel()\n",
        "output = model(X,a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7gsb_cCjQ9v"
      },
      "source": [
        "Your model should run smoothly.  The size of output should be 100 - the same as the minibatch size."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTvrTqrhjQ9v"
      },
      "source": [
        "# Training\n",
        "\n",
        "Next you need to define a loss function and then run gradient descend to learn the weights of the neural net.\n",
        "\n",
        "- There is a strong class imbalance problem in the training set (many more 0's than 1's).\n",
        "\n",
        "- To handle the class imbalance problem, we treat each sample differently.  Each data point is assigned a weight. Take a look at the variable named \"w\" in each data set.\n",
        "\n",
        "- Define a loss function, in which the total loss is a weighted combination of losses coming from each sample.  Use the weights specified in \"w\".   Note that this definition is different from our typical loss, where each sample contributes equally to the final total loss.\n",
        "\n",
        "- You should use binary_cross_entropy_with_logits with weight set to the weights per input element. Check out the documentation for details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YL2p42YTjQ9v"
      },
      "outputs": [],
      "source": [
        "###############################################################\n",
        "# complete this code block\n",
        "# define loss and optimizer\n",
        "###############################################################\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "###############################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_i1JcPUjQ9v"
      },
      "outputs": [],
      "source": [
        "# have a test run to check of your loss is defined properly\n",
        "loss = criterion(output, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tc4IBE9LjQ9v",
        "outputId": "fa5a9414-9e6a-4699-ecb5-09e626c30d23"
      },
      "outputs": [
        {
          "data": {
            "text/plain": "tensor(0.6550, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)"
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzuiBOuwjQ9w"
      },
      "source": [
        "\n",
        "## You task:\n",
        "\n",
        "You need to train the model on the training data, and use the **validation data** to select how many epochs you want to use and to choose the hidden dimension.\n",
        "\n",
        "Use the **weighted prediction accuracy** as the evaluation metric. That is, sum of the weights of the correct predictions divided by the total weight across all the input elements.\n",
        "\n",
        "Finally, report the **weighted accuracy on the test data.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "do4D1qUwjQ9w"
      },
      "source": [
        "### Run Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGKGYvqyjQ9w"
      },
      "outputs": [],
      "source": [
        "# Choose hyper parameters and optimizer\n",
        "num_epochs = 20\n",
        "learning_rate = 0.003\n",
        "optimizer = torch.optim.Adam(params=model.parameters(),lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VovO25g4jQ9w",
        "outputId": "798719ec-3dea-49ff-c157-0994ae0b5583"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/20], Step [100/2763], Loss: 2.6618\n",
            "Epoch [1/20], Step [200/2763], Loss: 0.7790\n",
            "Epoch [1/20], Step [300/2763], Loss: 0.3230\n",
            "Epoch [1/20], Step [400/2763], Loss: 0.3139\n",
            "Epoch [1/20], Step [500/2763], Loss: 0.2625\n",
            "Epoch [1/20], Step [600/2763], Loss: 0.2501\n",
            "Epoch [1/20], Step [700/2763], Loss: 0.2741\n",
            "Epoch [1/20], Step [800/2763], Loss: 0.2155\n",
            "Epoch [1/20], Step [900/2763], Loss: 1.0548\n",
            "Epoch [1/20], Step [1000/2763], Loss: 1.1388\n",
            "Epoch [1/20], Step [1100/2763], Loss: 0.2939\n",
            "Epoch [1/20], Step [1200/2763], Loss: 0.2649\n",
            "Epoch [1/20], Step [1300/2763], Loss: 0.2614\n",
            "Epoch [1/20], Step [1400/2763], Loss: 0.2494\n",
            "Epoch [1/20], Step [1500/2763], Loss: 0.2917\n",
            "Epoch [1/20], Step [1600/2763], Loss: 0.2443\n",
            "Epoch [1/20], Step [1700/2763], Loss: 0.2608\n",
            "Epoch [1/20], Step [1800/2763], Loss: 0.2678\n",
            "Epoch [1/20], Step [1900/2763], Loss: 0.2854\n",
            "Epoch [1/20], Step [2000/2763], Loss: 0.2344\n",
            "Epoch [1/20], Step [2100/2763], Loss: 0.2218\n",
            "Epoch [1/20], Step [2200/2763], Loss: 0.5205\n",
            "Epoch [1/20], Step [2300/2763], Loss: 0.2514\n",
            "Epoch [1/20], Step [2400/2763], Loss: 0.2167\n",
            "Epoch [1/20], Step [2500/2763], Loss: 0.2622\n",
            "Epoch [1/20], Step [2600/2763], Loss: 0.2700\n",
            "Epoch [1/20], Step [2700/2763], Loss: 0.2813\n",
            "Epoch [2/20], Step [100/2763], Loss: 1.8103\n",
            "Epoch [2/20], Step [200/2763], Loss: 1.1888\n",
            "Epoch [2/20], Step [300/2763], Loss: 0.2469\n",
            "Epoch [2/20], Step [400/2763], Loss: 0.6100\n",
            "Epoch [2/20], Step [500/2763], Loss: 0.2339\n",
            "Epoch [2/20], Step [600/2763], Loss: 0.2349\n",
            "Epoch [2/20], Step [700/2763], Loss: 0.2176\n",
            "Epoch [2/20], Step [800/2763], Loss: 0.2423\n",
            "Epoch [2/20], Step [900/2763], Loss: 0.2754\n",
            "Epoch [2/20], Step [1000/2763], Loss: 0.4639\n",
            "Epoch [2/20], Step [1100/2763], Loss: 0.3145\n",
            "Epoch [2/20], Step [1200/2763], Loss: 0.2963\n",
            "Epoch [2/20], Step [1300/2763], Loss: 0.2738\n",
            "Epoch [2/20], Step [1400/2763], Loss: 0.2533\n",
            "Epoch [2/20], Step [1500/2763], Loss: 0.2792\n",
            "Epoch [2/20], Step [1600/2763], Loss: 0.2636\n",
            "Epoch [2/20], Step [1700/2763], Loss: 1.3224\n",
            "Epoch [2/20], Step [1800/2763], Loss: 0.2677\n",
            "Epoch [2/20], Step [1900/2763], Loss: 0.2538\n",
            "Epoch [2/20], Step [2000/2763], Loss: 0.2472\n",
            "Epoch [2/20], Step [2100/2763], Loss: 0.2484\n",
            "Epoch [2/20], Step [2200/2763], Loss: 0.2190\n",
            "Epoch [2/20], Step [2300/2763], Loss: 0.2183\n",
            "Epoch [2/20], Step [2400/2763], Loss: 2.2409\n",
            "Epoch [2/20], Step [2500/2763], Loss: 2.0966\n",
            "Epoch [2/20], Step [2600/2763], Loss: 0.5712\n",
            "Epoch [2/20], Step [2700/2763], Loss: 3.0676\n",
            "Epoch [3/20], Step [100/2763], Loss: 0.2550\n",
            "Epoch [3/20], Step [200/2763], Loss: 0.2822\n",
            "Epoch [3/20], Step [300/2763], Loss: 0.2580\n",
            "Epoch [3/20], Step [400/2763], Loss: 0.2988\n",
            "Epoch [3/20], Step [500/2763], Loss: 0.4436\n",
            "Epoch [3/20], Step [600/2763], Loss: 0.4343\n",
            "Epoch [3/20], Step [700/2763], Loss: 0.2332\n",
            "Epoch [3/20], Step [800/2763], Loss: 0.4454\n",
            "Epoch [3/20], Step [900/2763], Loss: 0.2451\n",
            "Epoch [3/20], Step [1000/2763], Loss: 0.2781\n",
            "Epoch [3/20], Step [1100/2763], Loss: 0.8468\n",
            "Epoch [3/20], Step [1200/2763], Loss: 0.2461\n",
            "Epoch [3/20], Step [1300/2763], Loss: 0.1952\n",
            "Epoch [3/20], Step [1400/2763], Loss: 0.2662\n",
            "Epoch [3/20], Step [1500/2763], Loss: 1.1396\n",
            "Epoch [3/20], Step [1600/2763], Loss: 0.2726\n",
            "Epoch [3/20], Step [1700/2763], Loss: 0.2580\n",
            "Epoch [3/20], Step [1800/2763], Loss: 1.2723\n",
            "Epoch [3/20], Step [1900/2763], Loss: 0.1893\n",
            "Epoch [3/20], Step [2000/2763], Loss: 0.8201\n",
            "Epoch [3/20], Step [2100/2763], Loss: 0.2428\n",
            "Epoch [3/20], Step [2200/2763], Loss: 0.2810\n",
            "Epoch [3/20], Step [2300/2763], Loss: 0.2908\n",
            "Epoch [3/20], Step [2400/2763], Loss: 0.2739\n",
            "Epoch [3/20], Step [2500/2763], Loss: 0.2640\n",
            "Epoch [3/20], Step [2600/2763], Loss: 1.1926\n",
            "Epoch [3/20], Step [2700/2763], Loss: 0.2590\n",
            "Epoch [4/20], Step [100/2763], Loss: 0.2459\n",
            "Epoch [4/20], Step [200/2763], Loss: 0.7283\n",
            "Epoch [4/20], Step [300/2763], Loss: 0.4919\n",
            "Epoch [4/20], Step [400/2763], Loss: 0.2505\n",
            "Epoch [4/20], Step [500/2763], Loss: 0.2546\n",
            "Epoch [4/20], Step [600/2763], Loss: 0.2210\n",
            "Epoch [4/20], Step [700/2763], Loss: 0.2466\n",
            "Epoch [4/20], Step [800/2763], Loss: 1.0923\n",
            "Epoch [4/20], Step [900/2763], Loss: 0.2802\n",
            "Epoch [4/20], Step [1000/2763], Loss: 0.2593\n",
            "Epoch [4/20], Step [1100/2763], Loss: 0.2805\n",
            "Epoch [4/20], Step [1200/2763], Loss: 0.2191\n",
            "Epoch [4/20], Step [1300/2763], Loss: 0.9012\n",
            "Epoch [4/20], Step [1400/2763], Loss: 0.2758\n",
            "Epoch [4/20], Step [1500/2763], Loss: 0.2049\n",
            "Epoch [4/20], Step [1600/2763], Loss: 0.2270\n",
            "Epoch [4/20], Step [1700/2763], Loss: 0.2680\n",
            "Epoch [4/20], Step [1800/2763], Loss: 0.2085\n",
            "Epoch [4/20], Step [1900/2763], Loss: 0.2727\n",
            "Epoch [4/20], Step [2000/2763], Loss: 1.2335\n",
            "Epoch [4/20], Step [2100/2763], Loss: 0.2284\n",
            "Epoch [4/20], Step [2200/2763], Loss: 0.2424\n",
            "Epoch [4/20], Step [2300/2763], Loss: 0.2452\n",
            "Epoch [4/20], Step [2400/2763], Loss: 0.6904\n",
            "Epoch [4/20], Step [2500/2763], Loss: 0.3806\n",
            "Epoch [4/20], Step [2600/2763], Loss: 1.3835\n",
            "Epoch [4/20], Step [2700/2763], Loss: 0.2963\n",
            "Epoch [5/20], Step [100/2763], Loss: 1.3187\n",
            "Epoch [5/20], Step [200/2763], Loss: 0.2495\n",
            "Epoch [5/20], Step [300/2763], Loss: 0.2845\n",
            "Epoch [5/20], Step [400/2763], Loss: 0.2294\n",
            "Epoch [5/20], Step [500/2763], Loss: 0.2133\n",
            "Epoch [5/20], Step [600/2763], Loss: 1.5487\n",
            "Epoch [5/20], Step [700/2763], Loss: 0.2385\n",
            "Epoch [5/20], Step [800/2763], Loss: 0.2595\n",
            "Epoch [5/20], Step [900/2763], Loss: 0.2276\n",
            "Epoch [5/20], Step [1000/2763], Loss: 0.2551\n",
            "Epoch [5/20], Step [1100/2763], Loss: 0.2205\n",
            "Epoch [5/20], Step [1200/2763], Loss: 0.4102\n",
            "Epoch [5/20], Step [1300/2763], Loss: 0.1860\n",
            "Epoch [5/20], Step [1400/2763], Loss: 0.2584\n",
            "Epoch [5/20], Step [1500/2763], Loss: 0.2223\n",
            "Epoch [5/20], Step [1600/2763], Loss: 0.2163\n",
            "Epoch [5/20], Step [1700/2763], Loss: 0.2416\n",
            "Epoch [5/20], Step [1800/2763], Loss: 0.2643\n",
            "Epoch [5/20], Step [1900/2763], Loss: 0.2729\n",
            "Epoch [5/20], Step [2000/2763], Loss: 0.2676\n",
            "Epoch [5/20], Step [2100/2763], Loss: 0.2800\n",
            "Epoch [5/20], Step [2200/2763], Loss: 1.2653\n",
            "Epoch [5/20], Step [2300/2763], Loss: 0.3686\n",
            "Epoch [5/20], Step [2400/2763], Loss: 0.2955\n",
            "Epoch [5/20], Step [2500/2763], Loss: 0.2496\n",
            "Epoch [5/20], Step [2600/2763], Loss: 0.2594\n",
            "Epoch [5/20], Step [2700/2763], Loss: 0.2592\n",
            "Epoch [6/20], Step [100/2763], Loss: 1.8777\n",
            "Epoch [6/20], Step [200/2763], Loss: 0.8244\n",
            "Epoch [6/20], Step [300/2763], Loss: 0.2483\n",
            "Epoch [6/20], Step [400/2763], Loss: 0.2669\n",
            "Epoch [6/20], Step [500/2763], Loss: 0.2208\n",
            "Epoch [6/20], Step [600/2763], Loss: 0.2297\n",
            "Epoch [6/20], Step [700/2763], Loss: 0.2538\n",
            "Epoch [6/20], Step [800/2763], Loss: 0.8287\n",
            "Epoch [6/20], Step [900/2763], Loss: 0.2892\n",
            "Epoch [6/20], Step [1000/2763], Loss: 1.9172\n",
            "Epoch [6/20], Step [1100/2763], Loss: 0.2240\n",
            "Epoch [6/20], Step [1200/2763], Loss: 0.2614\n",
            "Epoch [6/20], Step [1300/2763], Loss: 0.2819\n",
            "Epoch [6/20], Step [1400/2763], Loss: 0.2814\n",
            "Epoch [6/20], Step [1500/2763], Loss: 0.2436\n",
            "Epoch [6/20], Step [1600/2763], Loss: 0.2675\n",
            "Epoch [6/20], Step [1700/2763], Loss: 0.2799\n",
            "Epoch [6/20], Step [1800/2763], Loss: 0.2234\n",
            "Epoch [6/20], Step [1900/2763], Loss: 0.2067\n",
            "Epoch [6/20], Step [2000/2763], Loss: 0.2251\n",
            "Epoch [6/20], Step [2100/2763], Loss: 0.2170\n",
            "Epoch [6/20], Step [2200/2763], Loss: 1.7458\n",
            "Epoch [6/20], Step [2300/2763], Loss: 1.7239\n",
            "Epoch [6/20], Step [2400/2763], Loss: 0.2600\n",
            "Epoch [6/20], Step [2500/2763], Loss: 0.6504\n",
            "Epoch [6/20], Step [2600/2763], Loss: 1.2900\n",
            "Epoch [6/20], Step [2700/2763], Loss: 0.2467\n",
            "Epoch [7/20], Step [100/2763], Loss: 0.2483\n",
            "Epoch [7/20], Step [200/2763], Loss: 0.1785\n",
            "Epoch [7/20], Step [300/2763], Loss: 1.8534\n",
            "Epoch [7/20], Step [400/2763], Loss: 0.5451\n",
            "Epoch [7/20], Step [500/2763], Loss: 0.2460\n",
            "Epoch [7/20], Step [600/2763], Loss: 0.2616\n",
            "Epoch [7/20], Step [700/2763], Loss: 0.3190\n",
            "Epoch [7/20], Step [800/2763], Loss: 0.2318\n",
            "Epoch [7/20], Step [900/2763], Loss: 0.2452\n",
            "Epoch [7/20], Step [1000/2763], Loss: 0.1876\n",
            "Epoch [7/20], Step [1100/2763], Loss: 0.2577\n",
            "Epoch [7/20], Step [1200/2763], Loss: 0.5798\n",
            "Epoch [7/20], Step [1300/2763], Loss: 0.2364\n",
            "Epoch [7/20], Step [1400/2763], Loss: 0.2306\n",
            "Epoch [7/20], Step [1500/2763], Loss: 0.2543\n",
            "Epoch [7/20], Step [1600/2763], Loss: 0.4580\n",
            "Epoch [7/20], Step [1700/2763], Loss: 0.2409\n",
            "Epoch [7/20], Step [1800/2763], Loss: 0.2943\n",
            "Epoch [7/20], Step [1900/2763], Loss: 0.2202\n",
            "Epoch [7/20], Step [2000/2763], Loss: 0.9973\n",
            "Epoch [7/20], Step [2100/2763], Loss: 0.2287\n",
            "Epoch [7/20], Step [2200/2763], Loss: 0.4227\n",
            "Epoch [7/20], Step [2300/2763], Loss: 0.2242\n",
            "Epoch [7/20], Step [2400/2763], Loss: 0.3267\n",
            "Epoch [7/20], Step [2500/2763], Loss: 0.2868\n",
            "Epoch [7/20], Step [2600/2763], Loss: 1.0627\n",
            "Epoch [7/20], Step [2700/2763], Loss: 0.2768\n",
            "Epoch [8/20], Step [100/2763], Loss: 0.4412\n",
            "Epoch [8/20], Step [200/2763], Loss: 0.2264\n",
            "Epoch [8/20], Step [300/2763], Loss: 1.9989\n",
            "Epoch [8/20], Step [400/2763], Loss: 0.2352\n",
            "Epoch [8/20], Step [500/2763], Loss: 0.2407\n",
            "Epoch [8/20], Step [600/2763], Loss: 1.0937\n",
            "Epoch [8/20], Step [700/2763], Loss: 0.3056\n",
            "Epoch [8/20], Step [800/2763], Loss: 0.2525\n",
            "Epoch [8/20], Step [900/2763], Loss: 0.2616\n",
            "Epoch [8/20], Step [1000/2763], Loss: 0.6452\n",
            "Epoch [8/20], Step [1100/2763], Loss: 0.2222\n",
            "Epoch [8/20], Step [1200/2763], Loss: 2.2505\n",
            "Epoch [8/20], Step [1300/2763], Loss: 0.2708\n",
            "Epoch [8/20], Step [1400/2763], Loss: 0.2536\n",
            "Epoch [8/20], Step [1500/2763], Loss: 0.4734\n",
            "Epoch [8/20], Step [1600/2763], Loss: 0.2260\n",
            "Epoch [8/20], Step [1700/2763], Loss: 0.2560\n",
            "Epoch [8/20], Step [1800/2763], Loss: 0.2221\n",
            "Epoch [8/20], Step [1900/2763], Loss: 0.7612\n",
            "Epoch [8/20], Step [2000/2763], Loss: 0.2232\n",
            "Epoch [8/20], Step [2100/2763], Loss: 0.2152\n",
            "Epoch [8/20], Step [2200/2763], Loss: 0.2681\n",
            "Epoch [8/20], Step [2300/2763], Loss: 0.2475\n",
            "Epoch [8/20], Step [2400/2763], Loss: 0.2229\n",
            "Epoch [8/20], Step [2500/2763], Loss: 0.2565\n",
            "Epoch [8/20], Step [2600/2763], Loss: 0.7273\n",
            "Epoch [8/20], Step [2700/2763], Loss: 0.2135\n",
            "Epoch [9/20], Step [100/2763], Loss: 0.2350\n",
            "Epoch [9/20], Step [200/2763], Loss: 0.2696\n",
            "Epoch [9/20], Step [300/2763], Loss: 0.6383\n",
            "Epoch [9/20], Step [400/2763], Loss: 0.2369\n",
            "Epoch [9/20], Step [500/2763], Loss: 2.0425\n",
            "Epoch [9/20], Step [600/2763], Loss: 0.9318\n",
            "Epoch [9/20], Step [700/2763], Loss: 0.2524\n",
            "Epoch [9/20], Step [800/2763], Loss: 0.2596\n",
            "Epoch [9/20], Step [900/2763], Loss: 0.2694\n",
            "Epoch [9/20], Step [1000/2763], Loss: 1.3510\n",
            "Epoch [9/20], Step [1100/2763], Loss: 2.4021\n",
            "Epoch [9/20], Step [1200/2763], Loss: 0.2467\n",
            "Epoch [9/20], Step [1300/2763], Loss: 1.2221\n",
            "Epoch [9/20], Step [1400/2763], Loss: 0.3053\n",
            "Epoch [9/20], Step [1500/2763], Loss: 0.2349\n",
            "Epoch [9/20], Step [1600/2763], Loss: 0.2310\n",
            "Epoch [9/20], Step [1700/2763], Loss: 3.2367\n",
            "Epoch [9/20], Step [1800/2763], Loss: 0.2286\n",
            "Epoch [9/20], Step [1900/2763], Loss: 0.2646\n",
            "Epoch [9/20], Step [2000/2763], Loss: 0.2396\n",
            "Epoch [9/20], Step [2100/2763], Loss: 2.0426\n",
            "Epoch [9/20], Step [2200/2763], Loss: 0.5550\n",
            "Epoch [9/20], Step [2300/2763], Loss: 0.2592\n",
            "Epoch [9/20], Step [2400/2763], Loss: 0.2468\n",
            "Epoch [9/20], Step [2500/2763], Loss: 0.2083\n",
            "Epoch [9/20], Step [2600/2763], Loss: 0.2112\n",
            "Epoch [9/20], Step [2700/2763], Loss: 0.2208\n",
            "Epoch [10/20], Step [100/2763], Loss: 0.2521\n",
            "Epoch [10/20], Step [200/2763], Loss: 0.2520\n",
            "Epoch [10/20], Step [300/2763], Loss: 0.6369\n",
            "Epoch [10/20], Step [400/2763], Loss: 0.2555\n",
            "Epoch [10/20], Step [500/2763], Loss: 1.4036\n",
            "Epoch [10/20], Step [600/2763], Loss: 0.2644\n",
            "Epoch [10/20], Step [700/2763], Loss: 0.2614\n",
            "Epoch [10/20], Step [800/2763], Loss: 0.7131\n",
            "Epoch [10/20], Step [900/2763], Loss: 0.1746\n",
            "Epoch [10/20], Step [1000/2763], Loss: 0.2091\n",
            "Epoch [10/20], Step [1100/2763], Loss: 0.6484\n",
            "Epoch [10/20], Step [1200/2763], Loss: 0.1963\n",
            "Epoch [10/20], Step [1300/2763], Loss: 0.2367\n",
            "Epoch [10/20], Step [1400/2763], Loss: 0.2340\n",
            "Epoch [10/20], Step [1500/2763], Loss: 0.1980\n",
            "Epoch [10/20], Step [1600/2763], Loss: 0.5042\n",
            "Epoch [10/20], Step [1700/2763], Loss: 0.2480\n",
            "Epoch [10/20], Step [1800/2763], Loss: 0.2698\n",
            "Epoch [10/20], Step [1900/2763], Loss: 0.6751\n",
            "Epoch [10/20], Step [2000/2763], Loss: 2.7300\n",
            "Epoch [10/20], Step [2100/2763], Loss: 0.2278\n",
            "Epoch [10/20], Step [2200/2763], Loss: 0.2455\n",
            "Epoch [10/20], Step [2300/2763], Loss: 0.2212\n",
            "Epoch [10/20], Step [2400/2763], Loss: 0.4500\n",
            "Epoch [10/20], Step [2500/2763], Loss: 0.2833\n",
            "Epoch [10/20], Step [2600/2763], Loss: 0.2051\n",
            "Epoch [10/20], Step [2700/2763], Loss: 0.2422\n",
            "Epoch [11/20], Step [100/2763], Loss: 0.2169\n",
            "Epoch [11/20], Step [200/2763], Loss: 0.2397\n",
            "Epoch [11/20], Step [300/2763], Loss: 0.4342\n",
            "Epoch [11/20], Step [400/2763], Loss: 0.1866\n",
            "Epoch [11/20], Step [500/2763], Loss: 0.2321\n",
            "Epoch [11/20], Step [600/2763], Loss: 0.5468\n",
            "Epoch [11/20], Step [700/2763], Loss: 0.2141\n",
            "Epoch [11/20], Step [800/2763], Loss: 0.2888\n",
            "Epoch [11/20], Step [900/2763], Loss: 0.2601\n",
            "Epoch [11/20], Step [1000/2763], Loss: 0.2824\n",
            "Epoch [11/20], Step [1100/2763], Loss: 0.2079\n",
            "Epoch [11/20], Step [1200/2763], Loss: 0.2348\n",
            "Epoch [11/20], Step [1300/2763], Loss: 0.2721\n",
            "Epoch [11/20], Step [1400/2763], Loss: 0.7259\n",
            "Epoch [11/20], Step [1500/2763], Loss: 2.8024\n",
            "Epoch [11/20], Step [1600/2763], Loss: 0.2458\n",
            "Epoch [11/20], Step [1700/2763], Loss: 0.2008\n",
            "Epoch [11/20], Step [1800/2763], Loss: 0.2744\n",
            "Epoch [11/20], Step [1900/2763], Loss: 3.0192\n",
            "Epoch [11/20], Step [2000/2763], Loss: 0.2476\n",
            "Epoch [11/20], Step [2100/2763], Loss: 0.9119\n",
            "Epoch [11/20], Step [2200/2763], Loss: 0.3109\n",
            "Epoch [11/20], Step [2300/2763], Loss: 0.2330\n",
            "Epoch [11/20], Step [2400/2763], Loss: 1.9526\n",
            "Epoch [11/20], Step [2500/2763], Loss: 1.0273\n",
            "Epoch [11/20], Step [2600/2763], Loss: 0.2427\n",
            "Epoch [11/20], Step [2700/2763], Loss: 1.2691\n",
            "Epoch [12/20], Step [100/2763], Loss: 0.2160\n",
            "Epoch [12/20], Step [200/2763], Loss: 0.1943\n",
            "Epoch [12/20], Step [300/2763], Loss: 0.3816\n",
            "Epoch [12/20], Step [400/2763], Loss: 0.2337\n",
            "Epoch [12/20], Step [500/2763], Loss: 0.2261\n",
            "Epoch [12/20], Step [600/2763], Loss: 0.2390\n",
            "Epoch [12/20], Step [700/2763], Loss: 0.2328\n",
            "Epoch [12/20], Step [800/2763], Loss: 0.2312\n",
            "Epoch [12/20], Step [900/2763], Loss: 0.2451\n",
            "Epoch [12/20], Step [1000/2763], Loss: 0.2543\n",
            "Epoch [12/20], Step [1100/2763], Loss: 0.2257\n",
            "Epoch [12/20], Step [1200/2763], Loss: 0.2622\n",
            "Epoch [12/20], Step [1300/2763], Loss: 0.2547\n",
            "Epoch [12/20], Step [1400/2763], Loss: 0.2253\n",
            "Epoch [12/20], Step [1500/2763], Loss: 0.1993\n",
            "Epoch [12/20], Step [1600/2763], Loss: 0.2257\n",
            "Epoch [12/20], Step [1700/2763], Loss: 0.2508\n",
            "Epoch [12/20], Step [1800/2763], Loss: 0.3827\n",
            "Epoch [12/20], Step [1900/2763], Loss: 0.6127\n",
            "Epoch [12/20], Step [2000/2763], Loss: 0.2530\n",
            "Epoch [12/20], Step [2100/2763], Loss: 0.3870\n",
            "Epoch [12/20], Step [2200/2763], Loss: 0.2862\n",
            "Epoch [12/20], Step [2300/2763], Loss: 0.3647\n",
            "Epoch [12/20], Step [2400/2763], Loss: 0.2189\n",
            "Epoch [12/20], Step [2500/2763], Loss: 0.2480\n",
            "Epoch [12/20], Step [2600/2763], Loss: 1.8447\n",
            "Epoch [12/20], Step [2700/2763], Loss: 2.0243\n",
            "Epoch [13/20], Step [100/2763], Loss: 0.3642\n",
            "Epoch [13/20], Step [200/2763], Loss: 0.5417\n",
            "Epoch [13/20], Step [300/2763], Loss: 0.2386\n",
            "Epoch [13/20], Step [400/2763], Loss: 0.2470\n",
            "Epoch [13/20], Step [500/2763], Loss: 0.2294\n",
            "Epoch [13/20], Step [600/2763], Loss: 0.2391\n",
            "Epoch [13/20], Step [700/2763], Loss: 0.2331\n",
            "Epoch [13/20], Step [800/2763], Loss: 0.2675\n",
            "Epoch [13/20], Step [900/2763], Loss: 0.2057\n",
            "Epoch [13/20], Step [1000/2763], Loss: 0.2515\n",
            "Epoch [13/20], Step [1100/2763], Loss: 0.2516\n",
            "Epoch [13/20], Step [1200/2763], Loss: 0.2360\n",
            "Epoch [13/20], Step [1300/2763], Loss: 0.2387\n",
            "Epoch [13/20], Step [1400/2763], Loss: 0.1913\n",
            "Epoch [13/20], Step [1500/2763], Loss: 0.2321\n",
            "Epoch [13/20], Step [1600/2763], Loss: 1.3372\n",
            "Epoch [13/20], Step [1700/2763], Loss: 0.2443\n",
            "Epoch [13/20], Step [1800/2763], Loss: 0.2426\n",
            "Epoch [13/20], Step [1900/2763], Loss: 0.2110\n",
            "Epoch [13/20], Step [2000/2763], Loss: 1.2326\n",
            "Epoch [13/20], Step [2100/2763], Loss: 0.4262\n",
            "Epoch [13/20], Step [2200/2763], Loss: 0.2287\n",
            "Epoch [13/20], Step [2300/2763], Loss: 0.2456\n",
            "Epoch [13/20], Step [2400/2763], Loss: 0.4250\n",
            "Epoch [13/20], Step [2500/2763], Loss: 0.2475\n",
            "Epoch [13/20], Step [2600/2763], Loss: 0.2470\n",
            "Epoch [13/20], Step [2700/2763], Loss: 0.2599\n",
            "Epoch [14/20], Step [100/2763], Loss: 0.2872\n",
            "Epoch [14/20], Step [200/2763], Loss: 0.2776\n",
            "Epoch [14/20], Step [300/2763], Loss: 0.2045\n",
            "Epoch [14/20], Step [400/2763], Loss: 0.3407\n",
            "Epoch [14/20], Step [500/2763], Loss: 0.2579\n",
            "Epoch [14/20], Step [600/2763], Loss: 0.2290\n",
            "Epoch [14/20], Step [700/2763], Loss: 0.1919\n",
            "Epoch [14/20], Step [800/2763], Loss: 0.1819\n",
            "Epoch [14/20], Step [900/2763], Loss: 2.0105\n",
            "Epoch [14/20], Step [1000/2763], Loss: 0.2042\n",
            "Epoch [14/20], Step [1100/2763], Loss: 0.1884\n",
            "Epoch [14/20], Step [1200/2763], Loss: 0.1846\n",
            "Epoch [14/20], Step [1300/2763], Loss: 0.2371\n",
            "Epoch [14/20], Step [1400/2763], Loss: 0.2257\n",
            "Epoch [14/20], Step [1500/2763], Loss: 0.2202\n",
            "Epoch [14/20], Step [1600/2763], Loss: 0.1931\n",
            "Epoch [14/20], Step [1700/2763], Loss: 0.1782\n",
            "Epoch [14/20], Step [1800/2763], Loss: 0.8852\n",
            "Epoch [14/20], Step [1900/2763], Loss: 1.2445\n",
            "Epoch [14/20], Step [2000/2763], Loss: 0.3107\n",
            "Epoch [14/20], Step [2100/2763], Loss: 2.4613\n",
            "Epoch [14/20], Step [2200/2763], Loss: 0.7339\n",
            "Epoch [14/20], Step [2300/2763], Loss: 0.2550\n",
            "Epoch [14/20], Step [2400/2763], Loss: 0.2610\n",
            "Epoch [14/20], Step [2500/2763], Loss: 0.2186\n",
            "Epoch [14/20], Step [2600/2763], Loss: 0.2423\n",
            "Epoch [14/20], Step [2700/2763], Loss: 0.7403\n",
            "Epoch [15/20], Step [100/2763], Loss: 0.2809\n",
            "Epoch [15/20], Step [200/2763], Loss: 0.2848\n",
            "Epoch [15/20], Step [300/2763], Loss: 0.2689\n",
            "Epoch [15/20], Step [400/2763], Loss: 1.1385\n",
            "Epoch [15/20], Step [500/2763], Loss: 0.2072\n",
            "Epoch [15/20], Step [600/2763], Loss: 1.5196\n",
            "Epoch [15/20], Step [700/2763], Loss: 0.2654\n",
            "Epoch [15/20], Step [800/2763], Loss: 0.2588\n",
            "Epoch [15/20], Step [900/2763], Loss: 0.2222\n",
            "Epoch [15/20], Step [1000/2763], Loss: 1.7453\n",
            "Epoch [15/20], Step [1100/2763], Loss: 0.2624\n",
            "Epoch [15/20], Step [1200/2763], Loss: 0.2603\n",
            "Epoch [15/20], Step [1300/2763], Loss: 0.2687\n",
            "Epoch [15/20], Step [1400/2763], Loss: 0.2506\n",
            "Epoch [15/20], Step [1500/2763], Loss: 0.7344\n",
            "Epoch [15/20], Step [1600/2763], Loss: 0.7442\n",
            "Epoch [15/20], Step [1700/2763], Loss: 0.2167\n",
            "Epoch [15/20], Step [1800/2763], Loss: 0.2669\n",
            "Epoch [15/20], Step [1900/2763], Loss: 0.2209\n",
            "Epoch [15/20], Step [2000/2763], Loss: 0.3069\n",
            "Epoch [15/20], Step [2100/2763], Loss: 0.1937\n",
            "Epoch [15/20], Step [2200/2763], Loss: 0.2320\n",
            "Epoch [15/20], Step [2300/2763], Loss: 0.2034\n",
            "Epoch [15/20], Step [2400/2763], Loss: 0.2473\n",
            "Epoch [15/20], Step [2500/2763], Loss: 0.5218\n",
            "Epoch [15/20], Step [2600/2763], Loss: 0.2440\n",
            "Epoch [15/20], Step [2700/2763], Loss: 0.2272\n",
            "Epoch [16/20], Step [100/2763], Loss: 0.2481\n",
            "Epoch [16/20], Step [200/2763], Loss: 0.2366\n",
            "Epoch [16/20], Step [300/2763], Loss: 0.2508\n",
            "Epoch [16/20], Step [400/2763], Loss: 0.2592\n",
            "Epoch [16/20], Step [500/2763], Loss: 0.2412\n",
            "Epoch [16/20], Step [600/2763], Loss: 0.2546\n",
            "Epoch [16/20], Step [700/2763], Loss: 0.2513\n",
            "Epoch [16/20], Step [800/2763], Loss: 1.1974\n",
            "Epoch [16/20], Step [900/2763], Loss: 0.2363\n",
            "Epoch [16/20], Step [1000/2763], Loss: 0.2737\n",
            "Epoch [16/20], Step [1100/2763], Loss: 0.2390\n",
            "Epoch [16/20], Step [1200/2763], Loss: 0.2025\n",
            "Epoch [16/20], Step [1300/2763], Loss: 0.2664\n",
            "Epoch [16/20], Step [1400/2763], Loss: 0.2357\n",
            "Epoch [16/20], Step [1500/2763], Loss: 0.2217\n",
            "Epoch [16/20], Step [1600/2763], Loss: 0.2214\n",
            "Epoch [16/20], Step [1700/2763], Loss: 0.1927\n",
            "Epoch [16/20], Step [1800/2763], Loss: 0.4424\n",
            "Epoch [16/20], Step [1900/2763], Loss: 0.2253\n",
            "Epoch [16/20], Step [2000/2763], Loss: 0.3125\n",
            "Epoch [16/20], Step [2100/2763], Loss: 0.2727\n",
            "Epoch [16/20], Step [2200/2763], Loss: 0.2304\n",
            "Epoch [16/20], Step [2300/2763], Loss: 0.2198\n",
            "Epoch [16/20], Step [2400/2763], Loss: 1.0852\n",
            "Epoch [16/20], Step [2500/2763], Loss: 0.2950\n",
            "Epoch [16/20], Step [2600/2763], Loss: 0.2890\n",
            "Epoch [16/20], Step [2700/2763], Loss: 0.2417\n",
            "Epoch [17/20], Step [100/2763], Loss: 1.0460\n",
            "Epoch [17/20], Step [200/2763], Loss: 0.2299\n",
            "Epoch [17/20], Step [300/2763], Loss: 1.3633\n",
            "Epoch [17/20], Step [400/2763], Loss: 2.4813\n",
            "Epoch [17/20], Step [500/2763], Loss: 0.9168\n",
            "Epoch [17/20], Step [600/2763], Loss: 0.1733\n",
            "Epoch [17/20], Step [700/2763], Loss: 0.2897\n",
            "Epoch [17/20], Step [800/2763], Loss: 0.2426\n",
            "Epoch [17/20], Step [900/2763], Loss: 0.5109\n",
            "Epoch [17/20], Step [1000/2763], Loss: 0.2285\n",
            "Epoch [17/20], Step [1100/2763], Loss: 0.2421\n",
            "Epoch [17/20], Step [1200/2763], Loss: 0.2214\n",
            "Epoch [17/20], Step [1300/2763], Loss: 0.8115\n",
            "Epoch [17/20], Step [1400/2763], Loss: 0.7128\n",
            "Epoch [17/20], Step [1500/2763], Loss: 0.2366\n",
            "Epoch [17/20], Step [1600/2763], Loss: 0.2680\n",
            "Epoch [17/20], Step [1700/2763], Loss: 0.1870\n",
            "Epoch [17/20], Step [1800/2763], Loss: 0.9800\n",
            "Epoch [17/20], Step [1900/2763], Loss: 0.2475\n",
            "Epoch [17/20], Step [2000/2763], Loss: 0.2020\n",
            "Epoch [17/20], Step [2100/2763], Loss: 0.2224\n",
            "Epoch [17/20], Step [2200/2763], Loss: 0.2240\n",
            "Epoch [17/20], Step [2300/2763], Loss: 1.8428\n",
            "Epoch [17/20], Step [2400/2763], Loss: 0.2716\n",
            "Epoch [17/20], Step [2500/2763], Loss: 0.2103\n",
            "Epoch [17/20], Step [2600/2763], Loss: 1.2914\n",
            "Epoch [17/20], Step [2700/2763], Loss: 0.3075\n",
            "Epoch [18/20], Step [100/2763], Loss: 0.2671\n",
            "Epoch [18/20], Step [200/2763], Loss: 0.3952\n",
            "Epoch [18/20], Step [300/2763], Loss: 0.2207\n",
            "Epoch [18/20], Step [400/2763], Loss: 1.9266\n",
            "Epoch [18/20], Step [500/2763], Loss: 1.7475\n",
            "Epoch [18/20], Step [600/2763], Loss: 0.2295\n",
            "Epoch [18/20], Step [700/2763], Loss: 0.2379\n",
            "Epoch [18/20], Step [800/2763], Loss: 0.5337\n",
            "Epoch [18/20], Step [900/2763], Loss: 0.2393\n",
            "Epoch [18/20], Step [1000/2763], Loss: 0.2631\n",
            "Epoch [18/20], Step [1100/2763], Loss: 0.2684\n",
            "Epoch [18/20], Step [1200/2763], Loss: 2.0698\n",
            "Epoch [18/20], Step [1300/2763], Loss: 0.2296\n",
            "Epoch [18/20], Step [1400/2763], Loss: 0.5499\n",
            "Epoch [18/20], Step [1500/2763], Loss: 1.3873\n",
            "Epoch [18/20], Step [1600/2763], Loss: 0.2576\n",
            "Epoch [18/20], Step [1700/2763], Loss: 3.2431\n",
            "Epoch [18/20], Step [1800/2763], Loss: 0.2233\n",
            "Epoch [18/20], Step [1900/2763], Loss: 0.2559\n",
            "Epoch [18/20], Step [2000/2763], Loss: 2.3873\n",
            "Epoch [18/20], Step [2100/2763], Loss: 0.2464\n",
            "Epoch [18/20], Step [2200/2763], Loss: 0.2087\n",
            "Epoch [18/20], Step [2300/2763], Loss: 0.4014\n",
            "Epoch [18/20], Step [2400/2763], Loss: 0.2918\n",
            "Epoch [18/20], Step [2500/2763], Loss: 0.7262\n",
            "Epoch [18/20], Step [2600/2763], Loss: 0.2710\n",
            "Epoch [18/20], Step [2700/2763], Loss: 0.2302\n",
            "Epoch [19/20], Step [100/2763], Loss: 0.1835\n",
            "Epoch [19/20], Step [200/2763], Loss: 0.2496\n",
            "Epoch [19/20], Step [300/2763], Loss: 0.2162\n",
            "Epoch [19/20], Step [400/2763], Loss: 1.2703\n",
            "Epoch [19/20], Step [500/2763], Loss: 0.1993\n",
            "Epoch [19/20], Step [600/2763], Loss: 0.2612\n",
            "Epoch [19/20], Step [700/2763], Loss: 0.2333\n",
            "Epoch [19/20], Step [800/2763], Loss: 1.1155\n",
            "Epoch [19/20], Step [900/2763], Loss: 0.1927\n",
            "Epoch [19/20], Step [1000/2763], Loss: 0.2299\n",
            "Epoch [19/20], Step [1100/2763], Loss: 0.6121\n",
            "Epoch [19/20], Step [1200/2763], Loss: 0.2739\n",
            "Epoch [19/20], Step [1300/2763], Loss: 0.2751\n",
            "Epoch [19/20], Step [1400/2763], Loss: 0.4068\n",
            "Epoch [19/20], Step [1500/2763], Loss: 1.1435\n",
            "Epoch [19/20], Step [1600/2763], Loss: 0.7864\n",
            "Epoch [19/20], Step [1700/2763], Loss: 1.5765\n",
            "Epoch [19/20], Step [1800/2763], Loss: 0.2277\n",
            "Epoch [19/20], Step [1900/2763], Loss: 1.2804\n",
            "Epoch [19/20], Step [2000/2763], Loss: 0.1935\n",
            "Epoch [19/20], Step [2100/2763], Loss: 1.1493\n",
            "Epoch [19/20], Step [2200/2763], Loss: 0.2234\n",
            "Epoch [19/20], Step [2300/2763], Loss: 0.2261\n",
            "Epoch [19/20], Step [2400/2763], Loss: 0.2581\n",
            "Epoch [19/20], Step [2500/2763], Loss: 1.3034\n",
            "Epoch [19/20], Step [2600/2763], Loss: 0.3002\n",
            "Epoch [19/20], Step [2700/2763], Loss: 0.2608\n",
            "Epoch [20/20], Step [100/2763], Loss: 0.2580\n",
            "Epoch [20/20], Step [200/2763], Loss: 0.2395\n",
            "Epoch [20/20], Step [300/2763], Loss: 0.2417\n",
            "Epoch [20/20], Step [400/2763], Loss: 0.2268\n",
            "Epoch [20/20], Step [500/2763], Loss: 0.9254\n",
            "Epoch [20/20], Step [600/2763], Loss: 1.4316\n",
            "Epoch [20/20], Step [700/2763], Loss: 0.2186\n",
            "Epoch [20/20], Step [800/2763], Loss: 0.2393\n",
            "Epoch [20/20], Step [900/2763], Loss: 0.2265\n",
            "Epoch [20/20], Step [1000/2763], Loss: 0.4241\n",
            "Epoch [20/20], Step [1100/2763], Loss: 0.2640\n",
            "Epoch [20/20], Step [1200/2763], Loss: 2.2389\n",
            "Epoch [20/20], Step [1300/2763], Loss: 1.1360\n",
            "Epoch [20/20], Step [1400/2763], Loss: 0.2423\n",
            "Epoch [20/20], Step [1500/2763], Loss: 0.2411\n",
            "Epoch [20/20], Step [1600/2763], Loss: 0.2602\n",
            "Epoch [20/20], Step [1700/2763], Loss: 0.2235\n",
            "Epoch [20/20], Step [1800/2763], Loss: 2.1963\n",
            "Epoch [20/20], Step [1900/2763], Loss: 0.2413\n",
            "Epoch [20/20], Step [2000/2763], Loss: 0.2158\n",
            "Epoch [20/20], Step [2100/2763], Loss: 0.2858\n",
            "Epoch [20/20], Step [2200/2763], Loss: 1.7199\n",
            "Epoch [20/20], Step [2300/2763], Loss: 2.8439\n",
            "Epoch [20/20], Step [2400/2763], Loss: 0.2520\n",
            "Epoch [20/20], Step [2500/2763], Loss: 0.2481\n",
            "Epoch [20/20], Step [2600/2763], Loss: 0.2495\n",
            "Epoch [20/20], Step [2700/2763], Loss: 0.2303\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "total_step = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (X, y, w, a) in enumerate(train_loader):\n",
        "        # images = images.to(device)\n",
        "        # labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        output = model(X, a)\n",
        "        criterion.weight = w\n",
        "        loss = criterion(output, y)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i+1) % 100 == 0:\n",
        "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
        "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run Validation"
      ],
      "metadata": {
        "collapsed": false,
        "id": "7eB7AlJwjQ9w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "import itertools\n",
        "\n",
        "y_pred_list = []\n",
        "y_target_list = []\n",
        "\n",
        "# Weights\n",
        "w_list = []\n",
        "\n",
        "model.eval()\n",
        "#Since we don't need model to back propagate the gradients in test set we use torch.no_grad()\n",
        "# reduces memory usage and speeds up computation\n",
        "with torch.no_grad():\n",
        "     for i, (X, y, w, a) in enumerate(valid_loader):\n",
        "        output = model(X, a)\n",
        "        y_pred_tag = (output>0).int()\n",
        "        y_pred_list.append(y_pred_tag.detach().numpy())\n",
        "        y_target_list.append(y.detach().numpy())\n",
        "\n",
        "        # Weights\n",
        "        w_list.append(w.detach().numpy())\n",
        "\n",
        "#Takes arrays and makes them list of list for each batch\n",
        "y_pred_list = [a.squeeze().tolist() for a in y_pred_list]\n",
        "#flattens the lists in sequence\n",
        "yvalid_pred = list(itertools.chain.from_iterable(y_pred_list))\n",
        "\n",
        "\n",
        "#Takes arrays and makes them list of list for each batch\n",
        "y_target_list = [a.squeeze().tolist() for a in y_target_list]\n",
        "#flattens the lists in sequence\n",
        "yvalid_target = list(itertools.chain.from_iterable(y_target_list))\n",
        "\n",
        "# Weights\n",
        "#Takes arrays and makes them list of list for each batch\n",
        "w_list = [a.squeeze().tolist() for a in w_list]\n",
        "#flattens the lists in sequence\n",
        "wvalid_list = list(itertools.chain.from_iterable(w_list))"
      ],
      "metadata": {
        "id": "XRdZ--hajQ9x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": "0.7210568051865643"
          },
          "execution_count": 371,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# define weighted prediction accuracy\n",
        "def weight_accuracy(predicted_y, true_y, weight):\n",
        "    return np.sum(np.array(weight)[np.array(predicted_y) == np.array(true_y)])/np.sum(np.array(weight))\n",
        "\n",
        "weight_accuracy(yvalid_pred,yvalid_target,wvalid_list)"
      ],
      "metadata": {
        "id": "wyiROGOmjQ9x",
        "outputId": "53be0c04-a46e-4efe-b3d3-13045ea502be"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7FLUU35jQ9x"
      },
      "source": [
        "## Final Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FH6NQ6WHjQ9x"
      },
      "source": [
        "### Generate predictions on test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ktfwCpHjQ9x"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "\n",
        "y_pred_list = []\n",
        "y_target_list = []\n",
        "\n",
        "# Weights\n",
        "w_list = []\n",
        "\n",
        "model.eval()\n",
        "#Since we don't need model to back propagate the gradients in test set we use torch.no_grad()\n",
        "# reduces memory usage and speeds up computation\n",
        "with torch.no_grad():\n",
        "     for i, (X, y, w, a) in enumerate(test_loader):\n",
        "        output = model(X, a)\n",
        "        y_pred_tag = (output>0).int()\n",
        "        y_pred_list.append(y_pred_tag.detach().numpy())\n",
        "        y_target_list.append(y.detach().numpy())\n",
        "        \n",
        "        # Weights\n",
        "        w_list.append(w.detach().numpy())\n",
        "\n",
        "#Takes arrays and makes them list of list for each batch\n",
        "y_pred_list = [a.squeeze().tolist() for a in y_pred_list]\n",
        "#flattens the lists in sequence\n",
        "ytest_pred = list(itertools.chain.from_iterable(y_pred_list))\n",
        "\n",
        "\n",
        "#Takes arrays and makes them list of list for each batch\n",
        "y_target_list = [a.squeeze().tolist() for a in y_target_list]\n",
        "#flattens the lists in sequence\n",
        "ytest_target = list(itertools.chain.from_iterable(y_target_list))\n",
        "\n",
        "# Weights\n",
        "#Takes arrays and makes them list of list for each batch\n",
        "w_list = [a.squeeze().tolist() for a in w_list]\n",
        "#flattens the lists in sequence\n",
        "wtest_list = list(itertools.chain.from_iterable(w_list))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqtvSyoNjQ9x"
      },
      "source": [
        "### Report\n",
        "- Precision\n",
        "- Recall\n",
        "- F1 Score\n",
        "- Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CdDYR4B6jQ9x",
        "outputId": "7274e8f4-5ab8-474a-f610-acaee157b1a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Confusion Matrix of the Test Set\n",
            "-----------\n",
            "[[29037  5345]\n",
            " [   57    89]]\n",
            "Precision of the MLP :\t0.016378358483621642\n",
            "Recall of the MLP    :\t0.6095890410958904\n",
            "F1 Score of the Model :\t0.031899641577060926\n"
          ]
        }
      ],
      "source": [
        "conf_matrix = confusion_matrix(ytest_target ,ytest_pred)\n",
        "print(\"Confusion Matrix of the Test Set\")\n",
        "print(\"-----------\")\n",
        "print(conf_matrix)\n",
        "print(\"Precision of the MLP :\\t\"+str(precision_score(ytest_target,ytest_pred)))\n",
        "print(\"Recall of the MLP    :\\t\"+str(recall_score(ytest_target,ytest_pred)))\n",
        "print(\"F1 Score of the Model :\\t\"+str(f1_score(ytest_target,ytest_pred)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pk3kgs3ijQ9y"
      },
      "source": [
        "##  Write code to **weighted prediction accuracy**\n",
        "\n",
        "That is, sum of the weights of the correct predictions divided by the total weight across all the input elements.\n",
        "\n",
        "Report the weighted accuracy on the test data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NizH878gjQ9y"
      },
      "outputs": [],
      "source": [
        "###########################################################\n",
        "#\n",
        "#  Complete the following function which calculates weight prediction accuracy\n",
        "#\n",
        "\n",
        "def weight_accuracy(predicted_y, true_y, weight):\n",
        "    return np.sum(np.array(weight)[np.array(predicted_y) == np.array(true_y)])/np.sum(np.array(weight))\n",
        "\n",
        "###########################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rpMkxgr2jQ9y",
        "outputId": "8d33d57a-6bf7-47df-e1ef-ef4e48c1171d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": "0.7270648918954576"
          },
          "execution_count": 375,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# calculate weighted accuracy on the test data\n",
        "weight_accuracy(ytest_pred,ytest_target,wtest_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZFnumsbjQ9y"
      },
      "source": [
        "# Problem 2 - CNN\n",
        "\n",
        "- Define a CNN model to solve the above problem.\n",
        "- The CNN model receives the same inputs. Instead of using MLP, it uses convolution to extract features.\n",
        "- Different from 2d images, the convolution will be 1d convolution\n",
        "- Again, use validation set to design your model and select hyperparamters.\n",
        "- Report weight accuracy on test set\n",
        "- Comment on your observations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [],
      "metadata": {
        "id": "TW_Xuxi4jQ9y"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyB6fzq8jQ9y"
      },
      "source": [
        "## CNN Details\n",
        "- For the CNN model, you have to use the 1D convolution module. The in_channels will be 4, one per DNA base. You can decide what number of out_channels and kernel size you want to use.\n",
        "\n",
        "- Check out torch Conv1d\n",
        "\n",
        "```\n",
        "torch.nn.Conv1d(in_channels, out_channels, kernel_size, stride=1, padding=0)\n",
        "```\n",
        "\n",
        "\n",
        "- Note the dimensions of the input required for the 1D convolution -- (N,C,L) a where N is a batch size, C denotes a number of channels, L is a length of signal sequence.\n",
        "\n",
        "- That is the input sequence has to be 4×101 and not 101×4 as in the data. So you should use torch.swapaxes function to swap the last two axes (not the batch axis) in the forward function.\n",
        "\n",
        "After the Conv1d, you can apply a relu activation, do dropout, and then try a maxpooling layer (1d). You can try more than one convolution layer too.\n",
        "\n",
        "Finally, flatten out the last convolution layer and use as input to an MLP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01-5eZt3jQ9y"
      },
      "outputs": [],
      "source": [
        "class MyConvModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MyConvModel, self).__init__()\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv1d(in_channels=4,out_channels=64,kernel_size=4,stride=1,padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.4),\n",
        "            nn.MaxPool1d(kernel_size=2,stride=2))\n",
        "\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv1d(in_channels=64,out_channels=128,kernel_size=4,stride=1,padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.4),\n",
        "            nn.MaxPool1d(kernel_size=2,stride=2))\n",
        "        \n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv1d(in_channels=128,out_channels=256,kernel_size=4,stride=1,padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.4),\n",
        "            nn.MaxPool1d(kernel_size=2,stride=2))\n",
        "        \n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        self.dense1 = nn.Sequential(\n",
        "            nn.LazyLinear(out_features=128),\n",
        "            nn.ReLU())\n",
        "        \n",
        "        self.dense2 = nn.Sequential(\n",
        "            nn.LazyLinear(out_features=128),\n",
        "            nn.ReLU())\n",
        "\n",
        "        self.output = nn.Sequential(\n",
        "            nn.BatchNorm1d(num_features=128+1),\n",
        "            nn.Linear(in_features=128+1, out_features=1))\n",
        "\n",
        "    def forward(self, X, a):\n",
        "        out = torch.transpose(X,1,2)\n",
        "        out = self.conv1(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.conv3(out)\n",
        "        out = self.flatten(out)\n",
        "        out = self.dense1(out)\n",
        "        #out = self.dense2(out)\n",
        "        out = torch.cat((out, a),1)\n",
        "        out = self.output(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "outputs": [],
      "source": [
        "# define weighted prediction accuracy\n",
        "def weight_accuracy(predicted_y, true_y, weight):\n",
        "    return np.sum(np.array(weight)[np.array(predicted_y) == np.array(true_y)]) / np.sum(np.array(weight))\n",
        "\n",
        "def accuracy_wrap(model,dataloder):\n",
        "    import itertools\n",
        "\n",
        "    y_pred_list = []\n",
        "    y_target_list = []\n",
        "\n",
        "    # Weights\n",
        "    w_list = []\n",
        "\n",
        "    model.eval()\n",
        "    #Since we don't need model to back propagate the gradients in test set we use torch.no_grad()\n",
        "    # reduces memory usage and speeds up computation\n",
        "    with torch.no_grad():\n",
        "        for i, (X, y, w, a) in enumerate(dataloder):\n",
        "            output = model(X.cuda(), a.cuda())\n",
        "            y_pred_tag = (output > 0).int()\n",
        "            y_pred_list.append(y_pred_tag.detach().to('cpu').numpy())\n",
        "            y_target_list.append(y.detach().to('cpu').numpy())\n",
        "\n",
        "        # Weights\n",
        "            w_list.append(w.detach().numpy())\n",
        "\n",
        "    #Takes arrays and makes them list of list for each batch\n",
        "    y_pred_list = [a.squeeze().tolist() for a in y_pred_list]\n",
        "    #flattens the lists in sequence\n",
        "    yvalid_pred = list(itertools.chain.from_iterable(y_pred_list))\n",
        "\n",
        "    #Takes arrays and makes them list of list for each batch\n",
        "    y_target_list = [a.squeeze().tolist() for a in y_target_list]\n",
        "    #flattens the lists in sequence\n",
        "    yvalid_target = list(itertools.chain.from_iterable(y_target_list))\n",
        "\n",
        "    # Weights\n",
        "    #Takes arrays and makes them list of list for each batch\n",
        "    w_list = [a.squeeze().tolist() for a in w_list]\n",
        "    #flattens the lists in sequence\n",
        "    wvalid_list = list(itertools.chain.from_iterable(w_list))\n",
        "\n",
        "    return weight_accuracy(yvalid_pred, yvalid_target, wvalid_list)\n",
        "\n"
      ],
      "metadata": {
        "id": "9f86ZopLjQ9y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conv_model = MyConvModel().cuda()"
      ],
      "metadata": {
        "id": "7omJdf8QnrvB",
        "outputId": "f82572b1-a28f-4251-c753-fb1b7df9fb0c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
            "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-130-2d93a4a699ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconv_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtotal_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'conv_model' is not defined"
          ]
        }
      ],
      "source": [
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# Choose hyper parameters and optimizer\n",
        "num_epochs = 20\n",
        "learning_rate = 0.001\n",
        "optimizer = torch.optim.Adam(params=conv_model.parameters(), lr=learning_rate)\n",
        "# Train the model\n",
        "total_step = len(train_loader)\n",
        "\n",
        "err_train = np.zeros(shape=num_epochs)\n",
        "err_valid = np.zeros(shape=num_epochs)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (X, y, w, a) in enumerate(train_loader):\n",
        "        # images = images.to(device)\n",
        "        # labels = labels.to(device)\n",
        "        X = X.cuda()\n",
        "        y = y.cuda()\n",
        "        w = w.cuda()\n",
        "        a = a.cuda()\n",
        "\n",
        "        # Forward pass\n",
        "        output = conv_model(X, a)\n",
        "        criterion.weight = w\n",
        "        loss = criterion(output, y)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    err_train[epoch] = accuracy_wrap(conv_model,train_loader)\n",
        "    err_valid[epoch] = accuracy_wrap(conv_model,valid_loader)\n",
        "\n",
        "    print('Epoch', epoch+1,'| train:',round(err_train[epoch],4) ,'| valid:',round(err_valid[epoch],4))"
      ],
      "metadata": {
        "id": "CNUny4UgjQ9z",
        "outputId": "2347d512-8b72-48e6-e34a-b1a55d6837d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.plot(err_train, color='r')\n",
        "plt.plot(err_valid, color='g')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "prNsAW8_jQ9z",
        "outputId": "c6dfe970-25f9-458a-c690-d938b10bf975",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0XTZI8yjQ9z"
      },
      "source": [
        "# Problem 3 - LSTM\n",
        "\n",
        "- For the LSTM model, use the encoder followed by a two layer MLP approach. That is, pass the input sequence (batch) through the LSTM and use the last hidden layer as the representation or embedding vector for the sequence. You can choose the dimensionality of the hidden layer. Next, use this vector as input to a two fully connected MLP layers -- the first connects the input vector to the hidden layer (again you can choose the size of the hidden layer), and the second connects the hidden to the output neuron. Use dropout and relu as appropriate.\n",
        "\n",
        "- Keep in mind that for the input to the LSTM module in pytorch use batch_first=True. This means that the batch dimension comes first, so the input is (N×101×4), which is how the input data is structured. Make note of the output of the LSTM layer so that you store the last hidden layer as the representation, to be used as input to the MLP layers.\n",
        "\n",
        "- Also, before feeding the output of the hidden layer to the output layer, you must concatenate the accessibility value. So if you are using hidden dimension of 128, then after concatenating the accessibility value, it will become a 129d vector, which should be fed to the final output layer of size 1, since we have a binary class/label.\n",
        "\n",
        "- You should use binary_cross_entropy_with_logits with weight set to the weights per input element.\n",
        "\n",
        "- You need to train the model on the training data, and use the validation data to select how many epochs you want to use and to choose the hidden dimension. Use the weighted prediction accuracy as the evaluation metric. That is, sum of the weights of the correct predictions divided by the total weight across all the input elements. Finally, report the weighted accuracy on the test data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "id": "NKhZCi6TjQ9z"
      },
      "outputs": [],
      "source": [
        "from numpy.core.fromnumeric import squeeze\n",
        "class MyRecuModel(nn.Module):\n",
        "    def __init__(self):\n",
        "      self.hidden_size = 128\n",
        "      self.layers = 1\n",
        "      super(MyRecuModel, self).__init__()\n",
        "      self.lstm = nn.LSTM(input_size = 4, hidden_size = self.hidden_size, \n",
        "                          num_layers = self.layers, batch_first=True, dropout=0.4)\n",
        "\n",
        "      self.dense = nn.Sequential(\n",
        "          nn.Linear(in_features = self.hidden_size, out_features=self.hidden_size),\n",
        "          nn.ReLU())\n",
        "        \n",
        "      self.output = nn.Sequential(\n",
        "          nn.Linear(in_features=self.hidden_size+1, out_features=1))\n",
        "\n",
        "    def forward(self, X, a):\n",
        "      batch_size = X.shape[0]\n",
        "\n",
        "      hidden = (torch.zeros(self.layers, batch_size, self.hidden_size).cuda(), \n",
        "                torch.zeros(self.layers, batch_size, self.hidden_size).cuda())\n",
        "\n",
        "      _, (final_hidden, final_cell) = self.lstm(X, hidden)\n",
        "\n",
        "      final_hidden = squeeze(final_hidden)\n",
        "\n",
        "      out = self.dense(final_hidden)\n",
        "      out = torch.cat((out, a),1)\n",
        "      out = self.output(out)\n",
        "      return out\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "recu_model = MyRecuModel().cuda()"
      ],
      "metadata": {
        "id": "u13fnxOuLcqQ",
        "outputId": "f8e4094c-aed9-4ca5-8a9f-4c2579bd379e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.4 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# Choose hyper parameters and optimizer\n",
        "num_epochs = 100\n",
        "learning_rate = 0.001\n",
        "optimizer = torch.optim.Adam(params=recu_model.parameters(), lr=learning_rate)\n",
        "# Train the model\n",
        "total_step = len(train_loader)\n",
        "\n",
        "err_train = np.zeros(shape=num_epochs)\n",
        "err_valid = np.zeros(shape=num_epochs)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    recu_model.train()\n",
        "\n",
        "    for i, (X, y, w, a) in enumerate(train_loader):\n",
        "\n",
        "        X = X.cuda()\n",
        "        y = y.cuda()\n",
        "        w = w.cuda()\n",
        "        a = a.cuda()\n",
        "\n",
        "        # Forward pass\n",
        "        output = recu_model(X, a)\n",
        "        criterion.weight = w\n",
        "        loss = criterion(output, y)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    err_train[epoch] = accuracy_wrap(recu_model,train_loader)\n",
        "    err_valid[epoch] = accuracy_wrap(recu_model,valid_loader)\n",
        "\n",
        "    print('Epoch', epoch+1,'| train:',round(err_train[epoch],4) ,'| valid:',round(err_valid[epoch],4))"
      ],
      "metadata": {
        "id": "p_JZoFGsLb_y",
        "outputId": "5cedbbcb-f5ca-449a-a98c-d84f9f8341db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | train: 0.5503 | valid: 0.5432\n",
            "Epoch 2 | train: 0.6259 | valid: 0.6164\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(err_train, color='r')\n",
        "plt.plot(err_valid, color='g')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kXSO50wq2W3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkafULaNjQ9z"
      },
      "source": [
        "# Statement of Collaboration\n",
        "\n",
        "It is mandatory to include a Statement of Collaboration in each submission, with respect to the guidelines below.\n",
        "Include the names of everyone involved in the discussions (especially in-person ones), and what was discussed.\n",
        "\n",
        "All students are required to follow the academic honesty guidelines posted on the course website.\n",
        "For programming assignments, in particular, I encourage the students to organize (perhaps using Campuswire) to discuss the task descriptions, requirements, bugs in my code, and the relevant technical content before they start working on it.\n",
        "\n",
        "However, you should not discuss the specific solutions, and, as a guiding principle, you are not allowed to take anything written or drawn away from these discussions (i.e. no photographs of the blackboard, written notes, referring to Campuswire, etc.).\n",
        "\n",
        "Especially after you have started working on the assignment, try to restrict the discussion to Campuswire as much as possible, so that there is no doubt as to the extent of your collaboration.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYSN8EmLjQ9z"
      },
      "source": [
        "##  Complete your statement of collaboration here:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGDgVdGMjQ9z"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGTQFOZcjQ9z"
      },
      "source": [
        "# What to submit\n",
        "- Export a notebook as PDF\n",
        "  - Go to Main menu | File and select Print . pdf.\n",
        "\n",
        "- Upload your jupyter notebook PDF on gradescope\n",
        "\n",
        "- The notebook must have output values for the final test accuracy.\n",
        "\n",
        "- Do not submit the data file or directories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [],
      "metadata": {
        "id": "1RHH0mzXjQ90"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}